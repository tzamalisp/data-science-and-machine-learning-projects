{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and Python\n",
    "\n",
    "using the *pyspark* library. This Jupyter Notebook serves as a reference code for the Big Data field and involves Amazon Web Services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a SparkContext\n",
    "\n",
    "Importing *SparkContext* from *pyspark* library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating SparkContext object\n",
    "\n",
    "SparkContext represents the connection to a Spark cluster, and can be used to create an RDD and broadcast variables on that cluster.\n",
    "\n",
    "SparkContext can be utilized at a time because of the way the things are built here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations\n",
    "\n",
    "Making a 'hello world' example, which is just reading a text file.\n",
    "___\n",
    "\n",
    "Creating a text file first, by using some special Jupyter Notebook command for this: %%writefile <file_name>.txt\n",
    "\n",
    "However, any text file can be read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%writefile text_file_example.txt\n",
    "first line\n",
    "second line\n",
    "third line\n",
    "fourth line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDD (Resilient Destributed Dataset)\n",
    "\n",
    "Loading the textfile using the **textFile** method of the SparkContext that was created. This method reads a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and returns it as an RDD of Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making an object\n",
    "textFileObject = sc.textFile('text_file_example.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparkâ€™s primary abstraction is a distributed collection of items called a Resilient Distributed Dataset (RDD). RDDs can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs.\n",
    "\n",
    "*textFileObject* is the RDD.\n",
    "\n",
    "*sc* is the SparkContext that connects to a Spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "An RDD has just been created by using the textFile method and operations on this object can be performed, such as counting the rows.\n",
    "\n",
    "RDDs have actions, which return values, and transformations, which return pointers to new RDDs. Here are a few actions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### counting the number of elements in RDD. Here, each line of the text file is an element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFileObject.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grabbing the first line, i.e. the first object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first line'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFileObject.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "Transformations can also be used, where more complicated tasks can take place. For example, the filter transformation will return a new RDD with a subset of items in the file. \n",
    "\n",
    "\n",
    "\n",
    "#### Creating a sample transformation using the filter() method:\n",
    "\n",
    "*This method (just like [Python's own filter function](https://www.w3schools.com/python/ref_func_filter.asp)) will only return elements that satisfy the condition.* \n",
    "\n",
    "Trying looking for lines that contain the word 'second'. *In that case, there should only be one line that has that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_find = textFileObject.filter(lambda line: 'second' in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how fast that was to run! The reason for that is because RDDs are lazily evaluated. That means that all those instructions of transformations don't actually execute until an action is performed! So, transformations are as a kind of recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking a look of what *second_find* is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD has a recipe of instructions to follow but it doesn't actually execute them until you ask for the performance of the action.\n",
    "\n",
    "#### Performing an action on the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['second line']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_find.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing another action on the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_find.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the transformations won't display an output and won't be run until an action is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
